defmodule Singularity.Jobs.EmbeddingFinetuneJob do
  @moduledoc """
  Daily Embedding Model Fine-tuning Job

  Runs fine-tuning on embedding models using pure Elixir (Nx + Axon).
  Executes daily at 2 AM, collects data from codebase, and fine-tunes models.

  ## Configuration

  Schedule in config:
  ```elixir
  config :singularity, Oban,
    queues: [training: 1],
    jobs: [
      {Singularity.Jobs.EmbeddingFinetuneJob, cron: "0 2 * * *"}  # 2 AM daily
    ]
  ```

  ## Process

  1. Collect code snippets from your codebase
  2. Create contrastive triplets (anchor, positive, negative)
  3. Fine-tune embedding model (Qodo or Jina v3)
  4. Save checkpoint with fine-tuned weights
  5. Reload weights in NxService (hot-swap)
  6. Verify new embeddings work correctly

  ## Manual Trigger

  ```elixir
  # Testing
  Singularity.Jobs.EmbeddingFinetuneJob.schedule_now()
  Singularity.Jobs.EmbeddingFinetuneJob.schedule_now(model: :qodo, epochs: 3)
  ```
  """

  use Oban.Worker, queue: :training, max_attempts: 1

  require Logger

  alias Singularity.Embedding.{Trainer, NxService}

  @impl Oban.Worker
  def perform(job) do
    Logger.info("ðŸš€ Starting embedding fine-tuning job")
    Logger.info("Timestamp: #{DateTime.utc_now()}")

    model = parse_atom(Map.get(job.args, "model", "qodo"))
    epochs = Map.get(job.args, "epochs", 1)
    learning_rate = Map.get(job.args, "learning_rate", 1.0e-5)
    batch_size = Map.get(job.args, "batch_size", 16)
    device = detect_device()

    with {:ok, training_data} <- collect_training_data(),
         {:ok, _} <- validate_training_data(training_data),
         {:ok, trainer} <- Trainer.new(model, device: device),
         {:ok, metrics} <-
           Trainer.train(trainer, training_data,
             epochs: epochs,
             learning_rate: learning_rate,
             batch_size: batch_size
           ),
         :ok <- NxService.reload_model(model, models_dir()),
         :ok <- verify_embeddings(model) do
      Logger.info("âœ… Fine-tuning completed successfully")
      Logger.info("Final metrics: #{inspect(metrics)}")
      :ok
    else
      {:error, reason} ->
        Logger.error("âŒ Fine-tuning failed: #{inspect(reason)}")
        {:error, reason}

      error ->
        Logger.error("âŒ Unexpected error: #{inspect(error)}")
        {:error, :unexpected_error}
    end
  end

  @doc """
  Manually trigger fine-tuning (useful for testing)
  """
  def schedule_now(_opts \\ []) do
    model = Keyword.get(opts, :model, :qodo)
    epochs = Keyword.get(opts, :epochs, 1)
    learning_rate = Keyword.get(opts, :learning_rate, 1.0e-5)
    batch_size = Keyword.get(opts, :batch_size, 16)

    job_args = %{
      "model" => Atom.to_string(model),
      "epochs" => epochs,
      "learning_rate" => learning_rate,
      "batch_size" => batch_size
    }

    Logger.info("Scheduling fine-tuning job: #{inspect(job_args)}")

    __MODULE__
    |> Oban.Job.new(job_args)
    |> Oban.insert!()
  end

  # Private helpers

  defp collect_training_data do
    Logger.info("Collecting training data from codebase...")

    try do
      # Step 1: Find all code files
      code_files = find_code_files()
      Logger.info("Found #{length(code_files)} code files")

      # Step 2: Extract code snippets
      snippets = extract_code_snippets(code_files)
      Logger.info("Extracted #{length(snippets)} code snippets")

      # Step 3: Create contrastive triplets using text similarity
      triplets = create_contrastive_triplets(snippets)
      Logger.info("Created #{length(triplets)} training triplets")

      if length(triplets) < 100 do
        Logger.error("Insufficient real training data: #{length(triplets)} triplets (need â‰¥100)")
        {:error, :insufficient_training_data}
      else
        Logger.info("Fine-tuning with #{length(triplets)} real training triplets")
        {:ok, triplets}
      end
    rescue
      e ->
        Logger.error("Error collecting training data: #{inspect(e)}")
        {:error, {:data_collection_error, e}}
    end
  end

  defp find_code_files do
    # Find all code files in key directories
    code_dirs = ["lib", "src", "priv"]
    extensions = [".ex", ".rs", ".py", ".js", ".ts", ".lua"]

    code_dirs
    |> Enum.flat_map(fn dir ->
      path = Path.join(File.cwd!(), dir)

      case File.dir?(path) do
        true ->
          path
          |> Path.join("**/*")
          |> Path.wildcard()
          |> Enum.filter(fn file ->
            File.regular?(file) and Enum.any?(extensions, &String.ends_with?(file, &1))
          end)
          |> Enum.filter(fn file ->
            # Filter out node_modules, build artifacts
            not String.contains?(file, ["node_modules", "dist", "_build", "priv/repo"])
          end)

        false ->
          []
      end
    end)
    |> Enum.uniq()
  end

  defp extract_code_snippets(files) do
    files
    |> Enum.flat_map(fn file ->
      case File.read(file) do
        {:ok, content} ->
          # Split by function/class definitions
          snippets = split_into_snippets(content, file)
          snippets

        {:error, _} ->
          []
      end
    end)
    |> Enum.filter(fn snippet ->
      # Filter snippets that are long enough to be meaningful
      String.length(snippet) >= 10 and String.length(snippet) <= 1000
    end)
    |> Enum.uniq()
  end

  defp split_into_snippets(content, _file) do
    # Split content by function/method definitions
    # Simple approach: split by common patterns
    patterns = [
      # Elixir/Ruby functions
      ~r/def\s+\w+[^}]*(?:\{|do)[^}]*/,
      # Anonymous functions
      ~r/fn[^}]*/,
      # Classes
      ~r/class\s+\w+[^}]*/,
      # Async functions
      ~r/async\s+fn[^}]*/,
      # Generic function signatures
      ~r/\w+\([^)]*\)\s*\{/
    ]

    Enum.flat_map(patterns, fn pattern ->
      Regex.scan(pattern, content)
      |> Enum.map(&List.first/1)
    end)
  end

  defp create_contrastive_triplets(snippets) do
    # Create triplets using text similarity (Jaccard distance)
    # Strategy:
    # 1. For each anchor snippet
    # 2. Find similar snippets (high Jaccard similarity)
    # 3. Find dissimilar snippets (low Jaccard similarity)
    # 4. Format as {anchor, positive, negative}

    count = min(100, div(length(snippets), 2))

    1..count
    |> Enum.map(fn _i ->
      # Randomly pick anchor
      anchor_idx = Enum.random(0..(length(snippets) - 1))
      anchor = Enum.at(snippets, anchor_idx)

      # Find similar snippet (high Jaccard)
      positive = find_similar_snippet(anchor, snippets, anchor_idx)

      # Find dissimilar snippet (low Jaccard)
      negative = find_dissimilar_snippet(anchor, snippets, anchor_idx)

      %{
        anchor: anchor,
        positive: positive,
        negative: negative
      }
    end)
    |> Enum.filter(fn triplet ->
      # Ensure all three are different
      triplet.anchor != triplet.positive and
        triplet.positive != triplet.negative and
        triplet.anchor != triplet.negative
    end)
  end

  defp find_similar_snippet(anchor, snippets, anchor_idx) do
    # Find snippet with high Jaccard similarity to anchor
    snippets
    |> Enum.with_index()
    |> Enum.reject(fn {_snippet, idx} -> idx == anchor_idx end)
    |> Enum.map(fn {snippet, _idx} ->
      similarity = jaccard_similarity(anchor, snippet)
      {snippet, similarity}
    end)
    |> Enum.sort_by(fn {_snippet, sim} -> -sim end)
    |> Enum.take(10)
    |> Enum.random()
    |> elem(0)
  rescue
    # Fallback if list is too small
    _e ->
      Enum.at(snippets, rem(anchor_idx + 1, length(snippets)))
  end

  defp find_dissimilar_snippet(anchor, snippets, anchor_idx) do
    # Find snippet with low Jaccard similarity to anchor
    snippets
    |> Enum.with_index()
    |> Enum.reject(fn {_snippet, idx} -> idx == anchor_idx end)
    |> Enum.map(fn {snippet, _idx} ->
      similarity = jaccard_similarity(anchor, snippet)
      {snippet, similarity}
    end)
    # Sort ascending (dissimilar first)
    |> Enum.sort_by(fn {_snippet, sim} -> sim end)
    |> Enum.take(10)
    |> Enum.random()
    |> elem(0)
  rescue
    # Fallback if list is too small
    _e ->
      Enum.at(snippets, rem(anchor_idx + 2, length(snippets)))
  end

  defp jaccard_similarity(text1, text2) do
    # Compute Jaccard similarity: |intersection| / |union|
    words1 = text1 |> String.split(~r/\W+/) |> MapSet.new() |> MapSet.filter(&(&1 != ""))
    words2 = text2 |> String.split(~r/\W+/) |> MapSet.new() |> MapSet.filter(&(&1 != ""))

    intersection = MapSet.intersection(words1, words2) |> MapSet.size()
    union = MapSet.union(words1, words2) |> MapSet.size()

    if union == 0 do
      0.0
    else
      intersection / union
    end
  end

  defp validate_training_data(data) do
    Logger.info("Validating training data...")

    # Check we have enough data
    if length(data) < 10 do
      {:error, "Not enough training data (need at least 10 triplets)"}
    else
      Logger.info("âœ… Validation passed (#{length(data)} triplets)")
      {:ok, data}
    end
  end

  defp verify_embeddings(model) do
    Logger.info("Verifying fine-tuned embeddings...")

    # Test with sample texts
    test_texts = [
      "def calculate(x): return x * 2",
      "async fn fetch() {}",
      "class MyClass: pass"
    ]

    case Enum.map(test_texts, fn text ->
           {:ok, _emb} = NxService.embed(text, model: model)
         end) do
      [_ | _] ->
        Logger.info("âœ… Embeddings verified successfully")
        :ok

      _ ->
        {:error, "Failed to generate embeddings"}
    end
  rescue
    _e ->
      Logger.warning("âš ï¸  Could not fully verify embeddings (model may not be loaded yet)")
      # Don't fail the job if verification has issues
      :ok
  end

  # Data generation

  defp generate_mock_triplets(count) do
    code_samples = [
      {"def hello", "def hi", "class Foo"},
      {"async fn main", "async fn start", "fn process"},
      {"SELECT * FROM", "SELECT all FROM", "INSERT INTO"},
      {"class MyClass", "class MyType", "def my_func"},
      {"fn process()", "fn handle()", "let x = 5"},
      {"import x from", "import y from", "export const z"},
      {"try: pass except", "try: do_something except", "if condition"},
      {"for item in list", "while running", "if ready"},
      {"@decorator\ndef func", "@wrapper\ndef method", "return value"},
      {"const API_URL", "const CONFIG", "const { x } = obj"}
    ]

    1..count
    |> Enum.map(fn i ->
      {anchor, positive, negative} = Enum.at(code_samples, rem(i, length(code_samples)))

      %{
        anchor: anchor <> "_#{i}",
        positive: positive <> "_#{i}",
        negative: negative <> "_#{i}"
      }
    end)
  end

  # Device detection

  defp detect_device do
    # Check for NVIDIA GPU
    case System.cmd("nvidia-smi", [], stderr_to_stdout: true) do
      {_output, 0} ->
        Logger.info("NVIDIA GPU detected, using CUDA")
        :cuda

      _ ->
        # Check for Apple Metal
        case :os.type() do
          {:unix, :darwin} ->
            Logger.info("macOS detected, using Metal (or CPU fallback)")
            # Metal support requires additional setup
            :cpu

          _ ->
            Logger.info("No GPU detected, using CPU")
            :cpu
        end
    end
  rescue
    _ ->
      Logger.info("GPU detection failed, using CPU")
      :cpu
  end

  # Helpers

  defp parse_atom(atom) when is_atom(atom), do: atom
  defp parse_atom(str) when is_binary(str), do: String.to_atom(str)
  defp parse_atom(_), do: :qodo

  defp models_dir do
    Path.join(File.cwd!(), "priv/models")
  end
end
