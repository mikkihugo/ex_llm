# Template: elixir_production_v2 v2.1.0 | Applied: 2025-01-12 | Upgrade: v2.1.0 -> v2.2.0
defmodule SingularityLLM.Service do
  @template_version "elixir_production_v2 v2.6.0"


  require Logger

  @capability_aliases %{
    "code" => "code",
    "codegen" => "code",
    "coding" => "code",
    "reasoning" => "reasoning",
    "analysis" => "reasoning",
    "architect" => "reasoning",
    "architecture" => "reasoning",
    "creativity" => "creativity",
    "creative" => "creativity",
    "design" => "creativity",
    "speed" => "speed",
    "fast" => "speed",
    "cost" => "cost",
    "cheap" => "cost"
  }
  @capability_values ["code", "reasoning", "creativity", "speed", "cost"]

  @type model :: String.t()
  @type message :: %{role: String.t(), content: String.t()}
  @type llm_request :: %{
          required(:messages) => [message()],
          optional(:model) => model(),
          optional(:provider) => String.t(),
          optional(:complexity) => String.t(),
          optional(:task_type) => String.t(),
          optional(:capabilities) => [String.t()],
          optional(:max_tokens) => non_neg_integer(),
          optional(:temperature) => float(),
          optional(:stream) => boolean()
        }
  @type llm_response :: %{
          text: String.t(),
          model: model(),
          tokens_used: non_neg_integer(),
          cost_cents: non_neg_integer()
        }

  # @calls: build_request/3 - Build LLM request structure
  # @calls: dispatch_request/2 - Enqueue request via Oban/pgmq
  # @calls: track_slo_metric/3 - Track SLO compliance
  # @calls: log_slo_breach/3 - Log SLA breaches
  # @telemetry: [:llm_service, :call, :start] - Call initiation
  # @telemetry: [:llm_service, :call, :stop] - Call completion
  # @slo: llm_call -> 2000ms
  @doc """
  Call an LLM via the Nexus queue with intelligent model selection and SLO monitoring.

  ## Parameters
  - model_or_complexity :: String.t() | atom() - Model name or complexity level
  - messages :: [message()] - List of conversation messages
  - opts :: keyword() - Optional parameters

  ## Returns
  - {:ok, llm_response()} - Successful response with metadata
  - {:error, reason()} - Error with specific reason

  ## Supported Options
  - :provider - Preferred provider hint (e.g. "claude", :gemini)
  - :complexity - Override inferred complexity (:simple, :medium, :complex)
  - :task_type - Task persona hint (:architect, :coder, :qa, etc.)
  - :capabilities - List of capability hints ([:code, :reasoning, :creativity])
  - :max_tokens, :temperature, :stream, :timeout - Standard request controls

  ## Examples

      # With specific model
      iex> SingularityLLM.Service.call("claude-sonnet-4.5", [%{role: "user", content: "Hello"}])
      {:ok, %{text: "Hello! How can I help you?", model: "claude-sonnet-4.5", tokens_used: 150, cost_cents: 1}}
      
      # With complexity level (model chosen by AI server)
      iex> SingularityLLM.Service.call(:simple, [%{role: "user", content: "Classify this"}])
      {:ok, %{text: "Classification result", model: "gemini-1.5-flash", tokens_used: 50, cost_cents: 1}}
      
      # Complex architecture with task metadata
      iex> SingularityLLM.Service.call(:complex, [%{role: "user", content: "Design a microservice"}], task_type: :architect)
      {:ok, %{text: "Architecture design...", model: "claude-3-5-sonnet-20241022", tokens_used: 2000, cost_cents: 50}}

      # Error handling
      iex> SingularityLLM.Service.call("invalid-model", [%{role: "user", content: "Test"}])
      {:error, :pgmq_error}
  """
  @spec call(model(), [message()], keyword()) :: {:ok, llm_response()} | {:error, term()}
  @spec call(atom(), [message()], keyword()) :: {:ok, llm_response()} | {:error, term()}
  def call(model_or_complexity, messages, opts \\ [])

  def call(model, messages, opts) when is_binary(model) do
    start_time = System.monotonic_time(:millisecond)
    correlation_id = generate_correlation_id()

    # Handle RCA session tracking (optional)
    session_result =
      Singularity.RCA.SessionManager.get_or_create_session(
        opts,
        %{
          initial_prompt: build_prompt_from_messages(messages),
          agent_id: Keyword.get(opts, :agent_id, "llm_service"),
          template_id: Keyword.get(opts, :template_id),
          agent_version: Keyword.get(opts, :agent_version, "v1.0.0")
        }
      )

    session_id =
      case session_result do
        {:ok, id} -> id
        {:error, _} -> nil
      end

    Logger.info("LLM call started", %{
      operation: :llm_call,
      correlation_id: correlation_id,
      session_id: session_id,
      model: model,
      message_count: length(messages),
      slo_target_ms: 2000
    })

    :telemetry.execute([:llm_service, :call, :start], %{
      model: model,
      message_count: length(messages),
      correlation_id: correlation_id,
      session_id: session_id
    })

    request =
      messages
      |> build_request(opts, %{model: model})

    case dispatch_request(request, opts) do
      {:ok, response} = result ->
        duration = System.monotonic_time(:millisecond) - start_time
        slo_status = if duration <= 2000, do: :within_sla, else: :sla_breach

        Logger.info("LLM call completed", %{
          operation: :llm_call,
          correlation_id: correlation_id,
          session_id: session_id,
          model: model,
          selected_model: Map.get(response, "model"),
          duration_ms: duration,
          slo_status: slo_status,
          tokens_used: Map.get(response, "tokens_used", 0),
          cost_cents: Map.get(response, "cost_cents", 0),
          success: true
        })

        :telemetry.execute([:llm_service, :call, :stop], %{
          model: model,
          duration: duration,
          slo_status: slo_status,
          tokens_used: Map.get(response, "tokens_used", 0),
          correlation_id: correlation_id,
          session_id: session_id
        })

        # Track SLO metrics
        track_slo_metric(:llm_call, duration, true)

        # Log SLO breach if needed
        if slo_status == :sla_breach do
          log_slo_breach(:llm_call, duration, 2000)
        end

        # Record metrics in RCA session if tracking
        if session_id do
          Singularity.RCA.SessionManager.record_generation_metrics(session_id, response)
        end

        result

      {:error, reason} = error ->
        duration = System.monotonic_time(:millisecond) - start_time

        Logger.error("LLM call failed", %{
          operation: :llm_call,
          correlation_id: correlation_id,
          session_id: session_id,
          model: model,
          error_reason: reason,
          duration_ms: duration,
          slo_status: :error,
          success: false
        })

        :telemetry.execute([:llm_service, :call, :exception], %{
          model: model,
          reason: reason,
          duration: duration,
          correlation_id: correlation_id,
          session_id: session_id
        })

        # Track SLO metrics for error case
        track_slo_metric(:llm_call, duration, false)

        # Record failure in RCA session if tracking
        if session_id do
          Singularity.RCA.SessionManager.complete_session(session_id, %{
            final_outcome: "failure_execution",
            failure_reason: inspect(reason)
          })
        end

        error
    end
  end

  def call(complexity, messages, opts) when complexity in [:simple, :medium, :complex] do
    opts = Keyword.put_new(opts, :complexity, complexity)

    request =
      messages
      |> build_request(opts)

    dispatch_request(request, opts)
  end

  def call(model, messages, opts) when is_atom(model) do
    model
    |> Atom.to_string()
    |> call(messages, opts)
  end

  @doc """
  Call LLM with a simple prompt string.

  ## Examples

      # With specific model
      iex> SingularityLLM.Service.call_with_prompt("claude-sonnet-4.5", "Hello world")
      {:ok, %{text: "Hello! How can I help you?", model: "claude-sonnet-4.5"}}
      
      # With complexity level
      iex> SingularityLLM.Service.call_with_prompt(:simple, "Hello world")
      {:ok, %{text: "Hello! How can I help you?", model: "gemini-1.5-flash"}}
  """
  @spec call_with_prompt(model() | atom(), String.t(), keyword()) ::
          {:ok, llm_response()} | {:error, term()}
  def call_with_prompt(model_or_complexity, prompt, opts \\ []) do
    messages = [%{role: "user", content: prompt}]
    call(model_or_complexity, messages, opts)
  end

  @doc """
  Call LLM with system prompt and user message.

  ## Examples

      # With specific model
      iex> SingularityLLM.Service.call_with_system("claude-sonnet-4.5", "You are a helpful assistant", "Hello")
      {:ok, %{text: "Hello! How can I help you?", model: "claude-sonnet-4.5"}}

      # With complexity level
      iex> SingularityLLM.Service.call_with_system(:complex, "You are a helpful assistant", "Hello")
      {:ok, %{text: "Hello! How can I help you?", model: "claude-3-5-sonnet-20241022"}}
  """
  @spec call_with_system(model() | atom(), String.t(), String.t(), keyword()) ::
          {:ok, llm_response()} | {:error, term()}
  def call_with_system(model_or_complexity, system_prompt, user_message, opts \\ []) do
    messages = [
      %{role: "system", content: system_prompt},
      %{role: "user", content: user_message}
    ]

    call(model_or_complexity, messages, opts)
  end

  @doc """
  Call LLM with Lua script for dynamic prompt building.

  Executes Lua script to build prompts with file reading, git integration,
  sub-prompts, and dynamic context assembly before calling LLM.

  ## Examples

      # With Lua script path
      iex> Service.call_with_script("sparc-specification.lua", %{
        requirements: "Build distributed chat system",
        project_root: "/app"
      })
      {:ok, %{text: "...", model: "claude-sonnet-4.5", script: "sparc-specification.lua"}}

      # With complexity override
      iex> Service.call_with_script("architecture-analysis.lua", context, complexity: :complex)
      {:ok, %{text: "...", model: "claude-3-5-sonnet-20241022"}}
  """
  @spec call_with_script(String.t(), map(), keyword()) :: {:ok, llm_response()} | {:error, term()}
  def call_with_script(script_path, context, opts \\ []) do
    # 1. Load Lua script from templates_data
    full_path = Path.join(["templates_data", "prompt_library", script_path])

    with {:ok, lua_code} <- File.read(full_path),
         {:ok, messages} <- LuaRunner.execute(lua_code, context) do
      # 2. Call LLM with assembled messages
      complexity = Keyword.get(opts, :complexity, :complex)

      case call(complexity, messages, opts) do
        {:ok, response} ->
          # 3. Add script metadata to response
          {:ok, Map.put(response, :script, script_path)}

        error ->
          error
      end
    else
      {:error, :enoent} ->
        Logger.error("Lua script not found: #{full_path}")
        {:error, {:script_not_found, script_path}}

      {:error, reason} ->
        Logger.error("Lua script execution failed: #{inspect(reason)}")
        {:error, {:script_error, reason}}
    end
  end

  @doc """
  Determine appropriate complexity level based on task characteristics.

  Automatically selects the right complexity level based on task type,
  optimizing for both cost and quality.

  Uses system-wide LLM.Config if provider is available, otherwise falls back
  to TaskTypeRegistry.

  ## Examples

      iex> Service.determine_complexity_for_task(:architect)
      :complex

      iex> Service.determine_complexity_for_task(:coder)
      :medium

      iex> Service.determine_complexity_for_task(:classifier)
      :simple

      iex> Service.determine_complexity_for_task(:unknown, default_complexity: :medium)
      :medium

      # With provider context (uses LLM.Config)
      iex> Service.determine_complexity_for_task(:architect, provider: "claude")
      :complex

  ## Task Type Mapping

  - **Complex:** :architect, :code_generation, :pattern_analyzer, :refactoring, :code_analysis, :qa
  - **Medium:** :coder, :decomposition, :planning, :pseudocode, :chat
  - **Simple:** :classifier, :parser, :simple_chat, :web_search
  """
  @spec determine_complexity_for_task(atom(), keyword()) :: :simple | :medium | :complex
  def determine_complexity_for_task(task_type, opts \\ [])

  def determine_complexity_for_task(task_type, opts) when is_atom(task_type) do
    # Try system-wide config if provider is available
    case Keyword.get(opts, :provider) do
      nil ->
        # Fallback to TaskTypeRegistry
        case Singularity.MetaRegistry.TaskTypeRegistry.get_complexity(task_type) do
          nil -> Keyword.get(opts, :default_complexity, :medium)
          complexity -> complexity
        end

      provider ->
        # Use system-wide LLM.Config
        context = %{task_type: task_type}

        case SingularityLLM.Config.get_task_complexity(provider, context) do
          {:ok, complexity} ->
            complexity

          {:error, _} ->
            # Fallback to TaskTypeRegistry if config fails
            case Singularity.MetaRegistry.TaskTypeRegistry.get_complexity(task_type) do
              nil -> Keyword.get(opts, :default_complexity, :medium)
              complexity -> complexity
            end
        end
    end
  end

  def determine_complexity_for_task(task_type, opts) when is_binary(task_type) do
    task_type
    |> String.to_atom()
    |> determine_complexity_for_task(opts)
  rescue
    ArgumentError ->
      Keyword.get(opts, :default_complexity, :medium)
  end

  def determine_complexity_for_task(_, opts) do
    Keyword.get(opts, :default_complexity, :medium)
  end

  @doc """
  Get available models.

  If provider is specified in opts, uses system-wide LLM.Config.
  Otherwise returns all default models.

  ## Examples

      iex> Service.get_available_models()
      ["claude-sonnet-4.5", "gemini-1.5-flash", ...]
      
      iex> Service.get_available_models(provider: "claude")
      {:ok, ["claude-3-5-sonnet-20241022", ...]}
  """
  @spec get_available_models(keyword()) :: [model()] | {:ok, [model()]} | {:error, term()}
  def get_available_models(opts \\ []) do
    case Keyword.get(opts, :provider) do
      nil ->
        # Return all default models
        [
          # Claude models
          "claude-sonnet-4.5",
          "claude-3-5-haiku-20241022",
          "claude-3-5-sonnet-20241022",
          "opus-4.1",
          # Gemini models
          "gemini-1.5-flash",
          "gemini-2.5-pro",
          # Codex models
          "gpt-5-codex",
          "o3-mini-codex",
          # Cursor models
          "cursor-auto",
          # Copilot models
          "github-copilot"
        ]

      provider ->
        # Use system-wide LLM.Config
        context = Map.new(Keyword.take(opts, [:task_type, :complexity]))
        SingularityLLM.Config.get_models(provider, context)
    end
  end

  defp build_request(messages, opts, overrides \\ %{}) do
    max_tokens = Keyword.get(opts, :max_tokens, 4000)
    temperature = Keyword.get(opts, :temperature, 0.7)
    stream = Keyword.get(opts, :stream, false)

    model =
      overrides[:model] ||
        opts
        |> Keyword.get(:model)
        |> normalize_string_option()

    provider =
      overrides[:provider] ||
        opts
        |> Keyword.get(:provider)
        |> normalize_provider()

    complexity =
      overrides[:complexity] ||
        opts
        |> Keyword.get(:complexity)
        |> normalize_complexity()

    task_type =
      overrides[:task_type] ||
        opts
        |> Keyword.get(:task_type)
        |> normalize_task_type_option()

    capabilities =
      overrides[:capabilities] ||
        opts
        |> Keyword.get(:capabilities)
        |> normalize_capabilities()

    %{
      messages: messages,
      max_tokens: max_tokens,
      temperature: temperature,
      stream: stream
    }
    |> maybe_put(:model, model)
    |> maybe_put(:provider, provider)
    |> maybe_put(:complexity, complexity)
    |> maybe_put(:task_type, task_type)
    |> maybe_put_capabilities(capabilities)
  end

  # LLM communication via Pgflow workflow system
  # @calls: Pgflow.Executor.execute/3 - Execute LlmRequest workflow synchronously
  # @error_flow: :timeout -> Workflow execution exceeded timeout threshold
  # @error_flow: :workflow_failed -> Pgflow workflow execution failed
  defp dispatch_request(request, opts) do
    # Execute LLM request via Pgflow workflow synchronously
    # This replaces both pgmq publishing and polling with direct workflow execution

    task_type = Map.get(request, :task_type, :medium)
    messages = Map.get(request, :messages, [])
    complexity = Map.get(request, :complexity, :medium)

    # Convert task_type to string for workflow
    task_type_str = if is_atom(task_type), do: Atom.to_string(task_type), else: task_type

    # Prepare workflow arguments
    workflow_args =
      %{
        "request_id" => Ecto.UUID.generate(),
        "task_type" => task_type_str,
        "messages" => messages,
        "model" => Keyword.get(opts, :model, "auto"),
        "provider" => Keyword.get(opts, :provider, "auto"),
        "api_version" => Keyword.get(opts, :api_version, "responses"),
        "complexity" => if(is_atom(complexity), do: Atom.to_string(complexity), else: complexity),
        "max_tokens" => Keyword.get(opts, :max_tokens),
        "temperature" => Keyword.get(opts, :temperature),
        "agent_id" => Keyword.get(opts, :agent_id),
        "previous_response_id" => Keyword.get(opts, :previous_response_id),
        "mcp_servers" => Keyword.get(opts, :mcp_servers),
        "store" => Keyword.get(opts, :store),
        "tools" => Keyword.get(opts, :tools)
      }
      # Remove nil values to keep message compact
      |> Enum.reject(fn {_k, v} -> is_nil(v) end)
      |> Map.new()

    Logger.info("Executing LLM workflow synchronously",
      task_type: task_type,
      complexity: complexity,
      message_count: length(messages)
    )

    # Execute the LLM workflow synchronously via Pgflow
    case Pgflow.Executor.execute(Singularity.Workflows.LlmRequest, workflow_args, Singularity.Repo, timeout: 30000) do
      {:ok, result} ->
        Logger.info("LLM workflow completed synchronously",
          task_type: task_type,
          complexity: complexity
        )

        # Return the actual LLM result
        {:ok, result}

      {:error, reason} ->
        Logger.error("LLM workflow execution failed",
          reason: reason,
          task_type: task_type,
          complexity: complexity
        )

        {:error, :workflow_failed}
    end
  end

  defp maybe_put(map, _key, nil), do: map
  defp maybe_put(map, key, value), do: Map.put(map, key, value)

  defp maybe_put_capabilities(map, []), do: map
  defp maybe_put_capabilities(map, caps), do: Map.put(map, :capabilities, caps)

  defp normalize_provider(value) do
    value
    |> normalize_string_option()
    |> case do
      nil -> nil
      string -> String.downcase(string)
    end
  end

  defp normalize_task_type_option(value) do
    value
    |> normalize_string_option()
    |> case do
      nil ->
        nil

      string ->
        string
        |> String.downcase()
        |> String.replace(~r/\s+/, "_")
    end
  end

  defp normalize_complexity(nil), do: :medium

  defp normalize_complexity(complexity) when is_atom(complexity) do
    case complexity do
      :simple -> :simple
      :medium -> :medium
      :complex -> :complex
      _ -> :medium
    end
  end

  defp normalize_complexity(complexity) when is_binary(complexity) do
    case String.downcase(complexity) do
      "simple" -> :simple
      "medium" -> :medium
      "complex" -> :complex
      _ -> :medium
    end
  end

  defp normalize_complexity(_), do: :medium

  defp normalize_capabilities(nil), do: []

  defp normalize_capabilities(capabilities) when is_list(capabilities) do
    capabilities
    |> Enum.map(&normalize_capability/1)
    |> Enum.reject(&is_nil/1)
    |> Enum.uniq()
  end

  defp normalize_capabilities(capabilities) when is_binary(capabilities) do
    capabilities
    |> String.split(",")
    |> Enum.map(&String.trim/1)
    |> normalize_capabilities()
  end

  defp normalize_capabilities(capabilities) when is_atom(capabilities) do
    [normalize_capability(capabilities)]
  end

  defp normalize_capabilities(_), do: []

  defp normalize_capability(capability) when is_atom(capability) do
    case capability do
      :code -> :code
      :reasoning -> :reasoning
      :creativity -> :creativity
      :analysis -> :analysis
      :synthesis -> :synthesis
      :planning -> :planning
      :problem_solving -> :problem_solving
      :communication -> :communication
      :learning -> :learning
      _ -> nil
    end
  end

  defp normalize_capability(capability) when is_binary(capability) do
    case String.downcase(String.trim(capability)) do
      "code" -> :code
      "reasoning" -> :reasoning
      "creativity" -> :creativity
      "analysis" -> :analysis
      "synthesis" -> :synthesis
      "planning" -> :planning
      "problem_solving" -> :problem_solving
      "problem-solving" -> :problem_solving
      "communication" -> :communication
      "learning" -> :learning
      _ -> nil
    end
  end

  defp normalize_capability(_), do: nil

  defp normalize_string_option(nil), do: nil

  defp normalize_string_option(option) when is_binary(option) do
    if String.trim(option) == "", do: nil, else: String.trim(option)
  end

  defp normalize_string_option(option) when is_atom(option) do
    Atom.to_string(option)
  end

  defp normalize_string_option(_), do: nil

  # SLO monitoring and Ericsson-style logging
  defp generate_correlation_id do
    :crypto.strong_rand_bytes(16) |> Base.encode64() |> binary_part(0, 22)
  end

  # Extract prompt from message list for RCA session tracking
  defp build_prompt_from_messages(messages) do
    messages
    |> Enum.filter(&(&1["role"] == "user"))
    |> Enum.map(& &1["content"])
    |> Enum.join("\n\n---\n\n")
    |> case do
      "" -> "No prompt provided"
      # Truncate to first 500 chars
      prompt -> String.slice(prompt, 0..500)
    end
  end

  defp track_slo_metric(operation, duration, success) do
    :telemetry.execute([:slo, :llm_operation, :complete], %{
      operation: operation,
      duration: duration,
      success: success,
      timestamp: System.system_time(:millisecond)
    })
  end

  defp log_slo_breach(operation, duration, threshold) do
    Logger.warning("SLO breach detected", %{
      operation: operation,
      duration_ms: duration,
      threshold_ms: threshold,
      breach_percentage: (duration / threshold * 100) |> Float.round(2),
      severity: :high
    })
  end
end
