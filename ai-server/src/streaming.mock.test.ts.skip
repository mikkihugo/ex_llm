import { describe, test, expect } from 'bun:test';
import { MockLanguageModelV3, simulateReadableStream } from 'ai/test';
import { streamText } from 'ai';

/**
 * Simplified AI SDK Streaming Tests
 *
 * Tests core streaming functionality using AI SDK's built-in MockLanguageModelV3
 */

describe('AI SDK Streaming (Refactored)', () => {
  test('basic text streaming', async () => {
    const mockModel = new MockLanguageModelV3({
      doStream: async () => ({
        stream: simulateReadableStream({
          chunks: [
            { type: 'text-delta', delta: 'Hello ', id: '1' },
            { type: 'text-delta', delta: 'world', id: '2' },
            { type: 'finish', finishReason: 'stop', totalUsage: { inputTokens: 10, outputTokens: 5, totalTokens: 15 } },
          ],
        }),
        rawCall: { rawPrompt: null, rawSettings: {} },
      }),
    });

    const result = streamText({
      model: mockModel,
      messages: [{ role: 'user', content: 'test' }],
    });

    // Consume stream manually to debug
    const chunks: string[] = [];
    for await (const chunk of result.textStream) {
      chunks.push(chunk);
    }

    expect(chunks.join('')).toBe('Hello world');

    const usage = await result.usage;
    expect(usage.promptTokens).toBe(10);  // AI SDK converts inputTokens → promptTokens
    expect(usage.completionTokens).toBe(5); // AI SDK converts outputTokens → completionTokens
  }, 10000); // 10s timeout

  test('streaming with delays', async () => {
    const mockModel = new MockLanguageModelV3({
      doStream: async () => ({
        stream: simulateReadableStream({
          chunks: [
            { type: 'text-delta', delta: 'First', id: '1' },
            { type: 'text-delta', delta: 'Second', id: '2' },
            { type: 'finish', finishReason: 'stop', totalUsage: { inputTokens: 5, outputTokens: 2, totalTokens: 7 } },
          ],
          chunkDelayInMs: 10,
        }),
        rawCall: { rawPrompt: null, rawSettings: {} },
      }),
    });

    const result = streamText({
      model: mockModel,
      messages: [{ role: 'user', content: 'test' }],
    });

    const startTime = Date.now();
    const text = await result.text;
    const elapsed = Date.now() - startTime;

    expect(text).toBe('FirstSecond');
    expect(elapsed).toBeGreaterThanOrEqual(20);
  });

  test('reports token usage accurately', async () => {
    const mockModel = new MockLanguageModelV3({
      doStream: async () => ({
        stream: simulateReadableStream({
          chunks: [
            { type: 'text-delta', delta: 'Response', id: '1' },
            {
              type: 'finish',
              finishReason: 'stop',
              usage: { inputTokens: 100, outputTokens: 50, totalTokens: 150 },
            },
          ],
        }),
        rawCall: { rawPrompt: null, rawSettings: {} },
      }),
    });

    const result = streamText({
      model: mockModel,
      messages: [{ role: 'user', content: 'test' }],
    });

    const usage = await result.usage;
    expect(usage.promptTokens).toBe(100);
    expect(usage.completionTokens).toBe(50);
    expect(usage.totalTokens).toBe(150);
  });

  test('handles different finish reasons', async () => {
    const reasons = ['stop', 'length', 'content-filter'] as const;

    for (const reason of reasons) {
      const mockModel = new MockLanguageModelV3({
        doStream: async () => ({
          stream: simulateReadableStream({
            chunks: [
              { type: 'text-delta', delta: 'Test', id: '1' },
              { type: 'finish', finishReason: reason, totalUsage: { inputTokens: 5, outputTokens: 2, totalTokens: 7 } },
            ],
          }),
          rawCall: { rawPrompt: null, rawSettings: {} },
        }),
      });

      const result = streamText({
        model: mockModel,
        messages: [{ role: 'user', content: 'test' }],
      });

      const finishReason = await result.finishReason;
      expect(finishReason).toBe(reason);
    }
  });

  test('tracks doStreamCalls for debugging', async () => {
    const mockModel = new MockLanguageModelV3({
      doStream: async () => ({
        stream: simulateReadableStream({
          chunks: [
            { type: 'text-delta', delta: 'Response', id: '1' },
            { type: 'finish', finishReason: 'stop', totalUsage: { inputTokens: 5, outputTokens: 2, totalTokens: 7 } },
          ],
        }),
        rawCall: { rawPrompt: null, rawSettings: {} },
      }),
    });

    await streamText({
      model: mockModel,
      messages: [{ role: 'user', content: 'test message' }],
    }).text;

    expect(mockModel.doStreamCalls).toHaveLength(1);
    expect(mockModel.doStreamCalls[0].prompt[0].content[0].text).toBe('test message');
  });

  test('simulates multiple providers with same interface', async () => {
    const providers = [
      { name: 'gemini-code', modelId: 'gemini-2.5-flash' },
      { name: 'claude-code', modelId: 'sonnet' },
      { name: 'github-copilot', modelId: 'gpt-4o' },
    ];

    for (const { name, modelId } of providers) {
      const mock = new MockLanguageModelV3({
        provider: name,
        modelId,
        doStream: async () => ({
          stream: simulateReadableStream({
            chunks: [
              { type: 'text-delta', delta: `Response from ${name}`, id: '1' },
              { type: 'finish', finishReason: 'stop', totalUsage: { inputTokens: 5, outputTokens: 3, totalTokens: 8 } },
            ],
          }),
          rawCall: { rawPrompt: null, rawSettings: {} },
        }),
      });

      const result = streamText({
        model: mock,
        messages: [{ role: 'user', content: 'test' }],
      });

      const text = await result.text;
      expect(text).toBe(`Response from ${name}`);
    }
  });
});

describe('OpenAI SSE Format Conversion', () => {
  async function convertToOpenAISSE(result: ReturnType<typeof streamText>) {
    const events: any[] = [];

    // Role chunk
    events.push({
      id: 'chatcmpl-test',
      object: 'chat.completion.chunk',
      choices: [{ index: 0, delta: { role: 'assistant' }, finish_reason: null }],
    });

    // Text chunks
    for await (const chunk of result.textStream) {
      events.push({
        id: 'chatcmpl-test',
        object: 'chat.completion.chunk',
        choices: [{ index: 0, delta: { content: chunk }, finish_reason: null }],
      });
    }

    // Usage chunk
    const usage = await result.usage;
    events.push({
      id: 'chatcmpl-test',
      object: 'chat.completion.chunk',
      choices: [{ index: 0, delta: {}, finish_reason: 'stop' }],
      usage: {
        prompt_tokens: usage.promptTokens,
        completion_tokens: usage.completionTokens,
        total_tokens: usage.totalTokens,
      },
    });

    return events;
  }

  test('converts AI SDK stream to OpenAI format', async () => {
    const mockModel = new MockLanguageModelV3({
      doStream: async () => ({
        stream: simulateReadableStream({
          chunks: [
            { type: 'text-delta', delta: 'Hello', id: '1' },
            { type: 'text-delta', delta: ' AI', id: '2' },
            { type: 'finish', finishReason: 'stop', totalUsage: { inputTokens: 3, outputTokens: 2, totalTokens: 5 } },
          ],
        }),
        rawCall: { rawPrompt: null, rawSettings: {} },
      }),
    });

    const result = streamText({
      model: mockModel,
      messages: [{ role: 'user', content: 'Hi' }],
    });

    const events = await convertToOpenAISSE(result);

    expect(events[0].choices[0].delta).toEqual({ role: 'assistant' });
    expect(events[1].choices[0].delta).toEqual({ content: 'Hello' });
    expect(events[2].choices[0].delta).toEqual({ content: ' AI' });
    expect(events[3].choices[0].finish_reason).toBe('stop');
    expect(events[3].usage).toEqual({
      prompt_tokens: 3,
      completion_tokens: 2,
      total_tokens: 5,
    });
  });

  test('OpenAI format has all required fields', async () => {
    const mockModel = new MockLanguageModelV3({
      doStream: async () => ({
        stream: simulateReadableStream({
          chunks: [
            { type: 'text-delta', delta: 'Test', id: '1' },
            { type: 'finish', finishReason: 'stop', totalUsage: { inputTokens: 1, outputTokens: 1, totalTokens: 2 } },
          ],
        }),
        rawCall: { rawPrompt: null, rawSettings: {} },
      }),
    });

    const result = streamText({
      model: mockModel,
      messages: [{ role: 'user', content: 'test' }],
    });

    const events = await convertToOpenAISSE(result);
    const chunk = events[1];

    expect(chunk).toHaveProperty('id');
    expect(chunk).toHaveProperty('object');
    expect(chunk.object).toBe('chat.completion.chunk');
    expect(chunk).toHaveProperty('choices');
    expect(chunk.choices[0]).toHaveProperty('index');
    expect(chunk.choices[0]).toHaveProperty('delta');
    expect(chunk.choices[0]).toHaveProperty('finish_reason');
  });
});

console.log('✅ All streaming tests use AI SDK built-in utilities');
console.log('   - MockLanguageModelV3 for providers');
console.log('   - simulateReadableStream for chunks');
console.log('   - No manual SSE formatting needed');
